# Actual runtime behavior of the reference С-implementation on client issues requests with `dc=203`

This analysis is derived directly from the control flow and data structures of the official C reference implementation. It describes the concrete runtime behavior at the transport and routing layers, not an abstract description of the MTProto protocol.

---

## 1. Extraction of the target DC from the initialization header

In the so-called “extended obfuscation mode” (`extmode2`), every client connection begins with a 64-byte initialization block (`random_header`) which, after decryption, contains several control parameters.

The relevant fields are located near the end of this buffer:

- Offset `+56`: 32-bit `tag`, used for mode identification and integrity verification.
- Offset `+60`: 16-bit integer (`short`), representing the requested target DC ID.

This value is immediately stored in the connection state:

```

TCP_RPC_DATA(connection)->extra_int4 = target_dc;

```

A critical detail is that this assignment happens exactly once during connection initialization and remains unchanged for the entire lifetime of the TCP session. There is no mechanism that later overrides this value based on payload contents.

In the specific case discussed here, this means:

```

TCP_RPC_DATA(c)->extra_int4 == 203

```

serves as the definitive routing reference for the session.

---

## 2. Role of `extra_int4` in the forwarding logic

Once the connection is established, every incoming MTProto frame follows the same forwarding path:

```

forward_mtproto_packet(...)
→ choose_proxy_target(TCP_RPC_DATA(C)->extra_int4)

```

Here, `extra_int4` acts as the primary key for selecting the destination cluster.

The function `choose_proxy_target()` implements the following decision logic:

1. Attempt to locate a cluster using:

```

mf_cluster_lookup(CurConf, target_dc, force=1)

```

2. If a cluster with ID 203 exists:
→ it is selected directly.

3. If no such cluster exists:
→ immediate fallback to the globally defined `default_cluster`.

The parameter `force=1` is important because it prevents immediate failure and enforces fallback behavior instead.

Afterward, a concrete backend connection is selected from the cluster:

- a random candidate is chosen,
- its state is validated (`connected`, not `closing`),
- this process may repeat up to five times if necessary.

This mechanism avoids routing traffic to dead or shutting-down backend connections.

---

## 3. Fully transparent forwarding without payload inspection

A fundamental architectural property of MTProxy is that it does not interpret MTProto payloads semantically.

Specifically, it does not perform:

- TL object parsing,
- RPC method inspection,
- container analysis,
- dynamic recomputation of the target DC.

The proxy operates strictly at the transport layer and treats the payload as opaque binary data.

Processing is limited to:

1. Removing the outer transport framing,
2. Associating the packet with an existing outbound RPC session,
3. Wrapping it into an internal `RPC_PROXY_REQ` frame,
4. Forwarding it over the corresponding backend TCP connection.

The payload itself remains bit-for-bit unchanged.

---

## 4. Behavior when no valid backend route is available

If no functional backend target can be found, a well-defined failure path is triggered.

Internally, the following occurs:

- The request is dropped,
- The global counter `dropped_queries` is incremented,
- The client connection is marked with:

```

RPC_F_DROPPED

```

This flag indicates a permanent routing failure condition.

Subsequently, this typically leads to:

- ignoring further packets from the client,
- eventual connection termination due to timeout or explicit closure.

Importantly, the proxy does not attempt automatic rerouting to a different DC. It strictly honors the originally specified target DC.

---

## 5. Return path of responses from backend to client

Response packets follow the reverse path, starting in the proxy’s RPC handler:

```

RPC_PROXY_ANS → mapping via out_conn_id

```

The proxy maintains an internal mapping table that associates each backend connection with its originating client session.

If a valid mapping exists:

```

client_send_message(...)

```

forwards the response unchanged to the client.

If no valid mapping exists (for example, if the client connection has already closed):

```

RPC_CLOSE_CONN

```

is sent to the backend to release resources and prevent orphaned sessions.

This ensures consistency between frontend and backend connection states.

---

## 6. Configuration-level control of routing for DC 203

Routing behavior is entirely determined by cluster definitions in the configuration.

Example:

```

proxy_for 203 149.154.x.x:443;

```

This creates a cluster with:

```

cluster_id = 203

```

and assigns the specified backend servers to it.

Additionally, a fallback cluster can be defined:

```

default 2;

```

This cluster will be used whenever no explicit mapping exists for a requested DC.

Without an explicit `proxy_for 203` directive, traffic targeting DC 203 is automatically routed through the default cluster.

---

## 7. Summary of effective system behavior

For a client connection specifying target DC 203, the behavior is deterministic:

- The DC ID is extracted once from the obfuscation initialization header.
- This value is stored in the connection state and never modified afterward.
- All traffic from this session is routed strictly based on this stored value.
- If a cluster with ID 203 exists, it is used.
- Otherwise, the default cluster is used as fallback.
- The MTProto payload contents never influence routing decisions.
- The proxy functions purely as a stateful transport-level forwarder without protocol-level interpretation.

As a result, the target DC is defined exclusively by the initialization header and remains invariant throughout the entire lifetime of the connection.
